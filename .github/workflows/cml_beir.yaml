name: benchmarks

on:
  workflow_dispatch:
  pull_request:
    types: [labeled]
jobs:
  deploy-runner:
    if: ${{ (github.event.action == 'labeled' && github.event.label.name == 'action:benchmark_beir') || github.event.action == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    steps:
      - uses: iterative/setup-cml@v1
      - uses: actions/checkout@v2
      - name: Deploy runner on EC2
        env:
          REPO_TOKEN: ${{ secrets.HAYSTACK_BOT_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_CI_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_CI_SECRET_ACCESS_KEY }}
          VPC: ${{ secrets.AWS_CI_VPC }}
        run: |
          cml runner \
              --cloud=aws \
              --cloud-region=us-east-1 \
              --cloud-type=p2.xlarge \
              --labels=cml-gpu \
              --cloud-aws-subnet=arn:aws:ec2:us-east-1:467229605384:subnet/subnet-07b7a46b564450513
  run-beir-benchmark:
    needs: deploy-runner
    runs-on: [ self-hosted, cml-gpu ]
    timeout-minutes: 60 # 72h
    container:
      image: docker://iterativeai/cml:0-dvc2-base1-gpu
      options: --gpus all
    steps:
      - uses: actions/checkout@v2
      - name: Run benchmark
        env:
          REPO_TOKEN: ${{ secrets.HAYSTACK_BOT_TOKEN }}
        run: |
          # Create CML report
          pip install .[elasticsearch,dev]
          echo "Some test from CML" >> report.md
          cml send-comment report.md


